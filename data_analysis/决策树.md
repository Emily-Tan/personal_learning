
决策树
 
  - 是一种类似流程图的树结构，其中，每个内部结点（非树叶结点）表示在一个属性上的测试，每个分支代表该测试的一个输出，而每个树叶结点（或终端结点）存放    一个类标号
  - 给定一个类标号未知的元组X，在决策树上测试该元组的属性值，跟踪一条由跟到叶结点的路径，该叶结点就存放着该元组的类预测。决策树容易转换成分类规则
  - ID3,C4.5和CART都采用贪心（即非回溯的）方法，其中决策树以自顶向下递归的分治方式构造。大多数决策树归纳算法都沿用这种自顶向下的方法，从训练元组集合    它们相关联的类标号开始构造决策树。随着树的构建，训练集递归的划分成较小的子集。
  - 属性选择度量：指定选择属性的启发式过程，用来选择可以按类“最好的”区分给定元组的属性。有信息增益，增益率，基尼指数等方法。另外树是否严格的二叉树也    由属性选择度量来确定。某些属性选择度量如基尼指数强制结果树是二叉树。其他度量，如信息增益并非如此，它允许多路划分（即从一个结点生长两个或多个分枝）
   按照属性选择度量即分裂准则指定分裂属性，并且也指出分裂点或分裂子集。理想情况下，分裂准则这样确定，使得每个分支上的输出分区尽可能的“纯”（一个分区是    纯的，如果它的所有元组都属于同一类）
- 决策算法之间的差别包括在创建树时如何选择属性和用于剪枝的机制。
- 在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问   题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不   纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。
-  CART回归树
　　  
   CART回归树和CART分类树的建立算法大部分是类似的，所以这里我们只讨论CART回归树和CART分类树的建立算法不同的地方。
      
　 首先，我们要明白，什么是回归树，什么是分类树。两者的区别在于样本输出，如果样本输出是离散值，那么这是一颗分类树。如果果样本输出是连续值，那么那么这    是一颗回归树。
   
　 除了概念的不同，CART回归树和CART分类树的建立和预测的区别主要有下面两点：
    
　　　1)连续值的处理方法不同
    
　　　2)决策树建立后做预测的方式不同。
    
   对于连续值的处理，我们知道CART分类树采用的是用基尼系数的大小来度量特征的各个划分点的优劣情况。这比较适合分类模型，但是对于回归模型，我们使用了常见    的和方差的度量方式，CART回归树的度量目标是，对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时    D1和D2的均方差之和最小所对应的特征和特征值划分点
    
   对于决策树建立后做预测的方式，上面讲到了CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的    均值或者中位数来预测输出结果。
      
 - 算法小结
   
　　上面我们对CART算法做了一个详细的介绍，CART算法相比C4.5算法的分类方法，采用了简化的二叉树模型，同时特征选择采用了近似的基尼系数来简化计算。当然CART树最大的好处是还可以做回归模型，这个C4.5没有。下表给出了ID3，C4.5和CART的一个比较总结。

         算法    支持模型   树结构    特征选择    连续值处理     缺失值处理    剪枝
         ID3      分类     多叉树    信息增益       不支持       不支持      不支持
         C4.5     分类     多叉树	信息增益比       支持        支持        支持
         CART   分类/回归  二叉树   基尼系数/均方差  支持         支持        支持
         
　　看起来CART算法高大上，那么CART算法还有没有什么缺点呢？有！主要的缺点我认为如下：
  
　　　1）应该大家有注意到，无论是ID3, C4.5还是CART,在做特征选择的时候都是选择最优的一个特征来做分类决策，但是大多数，分类决策不应该是由某一个特征决      定的，而是应该由一组特征决定的。这样决策得到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)。在选择最优特征的时候，      多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1，这里不多介绍。

　　　2）如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决。

   首先我们看看决策树算法的优点：
  
　　　　1）简单直观，生成的决策树很直观。

　　　　2）基本不需要预处理，不需要提前归一化，处理缺失值。

　　　　3）使用决策树预测的代价是O(log2m)O(log2m)。 m为样本数。

　　　　4）既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。

　　　　5）可以处理多维度输出的分类问题。

　　　　6）相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释

　　　　7）可以交叉验证的剪枝来选择模型，从而提高泛化能力。

　　　　8） 对于异常点的容错能力好，健壮性高。

　 我们再看看决策树算法的缺点:

　　　　1）决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。

　　　　2）决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。

　　　　3）寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习
        之类的方法来改善。

　　　　4）有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类
        方法来解决。

　　　　5）如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。



- 参考
  - <a href = "https://www.cnblogs.com/pinard/p/6050306.html">决策树算法原理(上)</a>
  - <a href = "https://www.cnblogs.com/pinard/p/6053344.html">决策树算法原理(下)</a>
  - <a href = "https://www.cnblogs.com/pinard/p/6056319.html">scikit-learn决策树算法类库使用小结</a>
