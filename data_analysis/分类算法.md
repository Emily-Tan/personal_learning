## 分类
 一 基本概念
 - 1 什么是分类：
   - 分类器  构造模型来预测类标号
   - 预测器  构造的模型是预测连续值函数或有序值，而不是类标号(回归分析是数值预测最常用的统计学方法)
  
    - 一般包含两个过程：
      - 学习阶段（构建分类模型）:建立描述预先定义的数据类或概念集的分类器，分类算法通过分析或从训练集"学习"来构造分类器
      - 和分类阶段（使用模型预测给定的类标号）
      
    是一种监督学习，即分类器的学习在被告知每个训练元祖属于哪个类的“监督”下进行 
    区别于无监督学习（或聚类），每个训练元组的类标号未知，并且要学习的类的个数或集合也可能事先不知道
      
    关于分类准确率：第二阶段首先要评估分类器的预测准确率。如果我们使用训练集来度量分类器的准确率，则评估可能是乐观的，因为分类器趋向于过分拟合该数据（即在学
    习期间，他可能包含了训练数据中的某些特定的异常，这些异常不一定在一般数据集中出现）。因此，需要使用由检验元组和他们相关联的类标号组成检验集（独立于训练元组）
    
    分类器在给定检验集上的准确率是分类器正确分类的检验元组所占的百分比。每个检验元组的类标号与学习模型对该元组的类预测进行比较。如果认为分类器的准确率可以接受，那么就可以用它对类标号未知的数据元组进行分类
 
 二 决策树
  - 是一种类似流程图的树结构，其中，每个内部结点（非树叶结点）表示在一个属性上的测试，每个分支代表该测试的一个输出，而每个树叶结点（或终端结点）存放一个类标号
  - 给定一个类标号未知的元组X，在决策树上测试该元组的属性值，跟踪一条由跟到叶结点的路径，该叶结点就存放着该元组的类预测。决策树容易转换成分类规则
  - ID3,C4.5和CART都采用贪心（即非回溯的）方法，其中决策树以自顶向下递归的分治方式构造。大多数决策树归纳算法都沿用这种自顶向下的方法，从训练元组集合它们相关联的类标号开始构造决策树。随着树的构建，训练集递归的划分成较小的子集。
  - 属性选择度量：指定选择属性的启发式过程，用来选择可以按类“最好的”区分给定元组的属性。有信息增益，增益率，基尼指数等方法。另外树是否严格的二叉树也由属性选择度量来确定。某些属性选择度量如基尼指数强制结果树是二叉树。其他度量，如信息增益并非如此，它允许多路划分（即从一个结点生长两个或多个分枝）
   按照属性选择度量即分裂准则指定分裂属性，并且也指出分裂点或分裂子集。理想情况下，分裂准则这样确定，使得每个分支上的输出分区尽可能的“纯”（一个分区是纯的，如果它的所有元组都属于同一类）
- 决策算法之间的差别包括在创建树时如何选择属性和用于剪枝的机制。


- 参考
  - <a href = "https://www.cnblogs.com/pinard/p/6050306.html">决策树算法原理(上)</a>
  - <a href = "https://www.cnblogs.com/pinard/p/6053344.html">决策树算法原理(下)</a>
  - <a href = "https://www.cnblogs.com/pinard/p/6056319.html">scikit-learn决策树算法类库使用小结</a>
