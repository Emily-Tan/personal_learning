## 分类
 一 基本概念
    
   - 什么是分类：
      - 分类器  构造模型来预测类标号
      - 预测器  构造的模型是预测连续值函数或有序值，而不是类标号(回归分析是数值预测最常用的统计学方法)
  
   - 一般包含两个过程：
      - 学习阶段（构建分类模型）:建立描述预先定义的数据类或概念集的分类器，分类算法通过分析或从训练集"学习"来构造分类器
      - 和分类阶段（使用模型预测给定的类标号）
      
   是一种监督学习，即分类器的学习在被告知每个训练元祖属于哪个类的“监督”下进行 
   区别于无监督学习（或聚类），每个训练元组的类标号未知，并且要学习的类的个数或集合也可能事先不知道
      
  关于分类准确率：第二阶段首先要评估分类器的预测准确率。如果我们使用训练集来度量分类器的准确率，则评估可能是乐观的，因为分类器趋向于过分拟合该数据（即在学习期间，他可能包含了训练数据中的某些特定的异常，这些异常不一定在一般数据集中出现）。因此，需要使用由检验元组和他们相关联的类标号组成检验集（独  立于训练元组）
    
 分类器在给定检验集上的准确率是分类器正确分类的检验元组所占的百分比。每个检验元组的类标号与学习模型对该元组的类预测进行比较。如果认为分类器的准确率可以接受，那么就可以用它对类标号未知的数据元组进行分类
 
二 决策树
 
  - 是一种类似流程图的树结构，其中，每个内部结点（非树叶结点）表示在一个属性上的测试，每个分支代表该测试的一个输出，而每个树叶结点（或终端结点）存放    一个类标号
  - 给定一个类标号未知的元组X，在决策树上测试该元组的属性值，跟踪一条由跟到叶结点的路径，该叶结点就存放着该元组的类预测。决策树容易转换成分类规则
  - ID3,C4.5和CART都采用贪心（即非回溯的）方法，其中决策树以自顶向下递归的分治方式构造。大多数决策树归纳算法都沿用这种自顶向下的方法，从训练元组集合    它们相关联的类标号开始构造决策树。随着树的构建，训练集递归的划分成较小的子集。
  - 属性选择度量：指定选择属性的启发式过程，用来选择可以按类“最好的”区分给定元组的属性。有信息增益，增益率，基尼指数等方法。另外树是否严格的二叉树也    由属性选择度量来确定。某些属性选择度量如基尼指数强制结果树是二叉树。其他度量，如信息增益并非如此，它允许多路划分（即从一个结点生长两个或多个分枝）
   按照属性选择度量即分裂准则指定分裂属性，并且也指出分裂点或分裂子集。理想情况下，分裂准则这样确定，使得每个分支上的输出分区尽可能的“纯”（一个分区是    纯的，如果它的所有元组都属于同一类）
- 决策算法之间的差别包括在创建树时如何选择属性和用于剪枝的机制。
- 在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问   题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不   纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。
-  CART回归树
　　  
   CART回归树和CART分类树的建立算法大部分是类似的，所以这里我们只讨论CART回归树和CART分类树的建立算法不同的地方。
      
　 首先，我们要明白，什么是回归树，什么是分类树。两者的区别在于样本输出，如果样本输出是离散值，那么这是一颗分类树。如果果样本输出是连续值，那么那么这    是一颗回归树。
   
　 除了概念的不同，CART回归树和CART分类树的建立和预测的区别主要有下面两点：
    
　　　1)连续值的处理方法不同
    
　　　2)决策树建立后做预测的方式不同。
    
   对于连续值的处理，我们知道CART分类树采用的是用基尼系数的大小来度量特征的各个划分点的优劣情况。这比较适合分类模型，但是对于回归模型，我们使用了常见    的和方差的度量方式，CART回归树的度量目标是，对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时    D1和D2的均方差之和最小所对应的特征和特征值划分点
    
   对于决策树建立后做预测的方式，上面讲到了CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的    均值或者中位数来预测输出结果。
      
 - 算法小结
   
　　上面我们对CART算法做了一个详细的介绍，CART算法相比C4.5算法的分类方法，采用了简化的二叉树模型，同时特征选择采用了近似的基尼系数来简化计算。当然CART树最大的好处是还可以做回归模型，这个C4.5没有。下表给出了ID3，C4.5和CART的一个比较总结。

         算法    支持模型   树结构    特征选择    连续值处理     缺失值处理    剪枝
         ID3      分类     多叉树    信息增益       不支持       不支持      不支持
         C4.5     分类     多叉树	信息增益比       支持        支持        支持
         CART   分类/回归  二叉树   基尼系数/均方差  支持         支持        支持
         
　　看起来CART算法高大上，那么CART算法还有没有什么缺点呢？有！主要的缺点我认为如下：
  
　　　1）应该大家有注意到，无论是ID3, C4.5还是CART,在做特征选择的时候都是选择最优的一个特征来做分类决策，但是大多数，分类决策不应该是由某一个特征决      定的，而是应该由一组特征决定的。这样决策得到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)。在选择最优特征的时候，      多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1，这里不多介绍。

　　　2）如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决。

   首先我们看看决策树算法的优点：
  
　　　　1）简单直观，生成的决策树很直观。

　　　　2）基本不需要预处理，不需要提前归一化，处理缺失值。

　　　　3）使用决策树预测的代价是O(log2m)O(log2m)。 m为样本数。

　　　　4）既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。

　　　　5）可以处理多维度输出的分类问题。

　　　　6）相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释

　　　　7）可以交叉验证的剪枝来选择模型，从而提高泛化能力。

　　　　8） 对于异常点的容错能力好，健壮性高。

　 我们再看看决策树算法的缺点:

　　　　1）决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。

　　　　2）决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。

　　　　3）寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习
        之类的方法来改善。

　　　　4）有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类
        方法来解决。

　　　　5）如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。



- 参考
  - <a href = "https://www.cnblogs.com/pinard/p/6050306.html">决策树算法原理(上)</a>
  - <a href = "https://www.cnblogs.com/pinard/p/6053344.html">决策树算法原理(下)</a>
  - <a href = "https://www.cnblogs.com/pinard/p/6056319.html">scikit-learn决策树算法类库使用小结</a>
  
三  K-最近邻法(k-nearest neighbors,KNN)
  - KNN做回归和分类的主要区别在于最后做预测时候的决策方式不同。KNN做分类预测时，一般是选择多数表决法，即训练集里和预测的样本特征最近的K个样本，预测     为里面有最多类别数的类别。而KNN做回归时，一般是选择平均法，即最近的K个样本的样本输出的平均值作为回归预测值
  - scikit-learn里只使用了蛮力实现(brute-force)，KD树实现(KDTree)和球树(BallTree)实现。其余的实现方法比如BBF树，MVP树等
  - KNN算法三要素
  
     KNN算法我们主要要考虑三个重要的要素，对于固定的训练集，只要这三点确定了，算法的预测方式也就决定了。这三个最终的要素是k值的选取，距离度量的方式和      分类决策规则。
     
     对于分类决策规则，一般都是使用前面提到的多数表决法。所以我们重点是关注与k值的选择和距离的度量方式。
   - 对于k值的选择，没有一个固定的经验，一般根据样本的分布，选择一个较小的值，可以通过交叉验证选择一个合适的k值。
   
     选择较小的k值，就相当于用较小的领域中的训练实例进行预测，训练误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的     问题是泛化误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；
     
　　 选择较大的k值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少泛化误差，但缺点是训练误差会增大。这时候，与输入实例较远（不相似的）训练     实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。
　  一个极端是k等于样本数m，则完全没有分类，此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单。
  - 对于距离的度量，我们有很多的距离度量方式，但是最常用的是欧式距离，曼哈顿距离，闵可夫斯基距离(Minkowski Distance)（公式参考链接）
  - KNN算法小结
  
  　KNN的主要优点有：

　　　　1） 理论成熟，思想简单，既可以用来做分类也可以用来做回归

　　　　2） 可用于非线性分类

　　　　3） 训练时间复杂度比支持向量机之类的算法低，仅为O(n)

　　　　4） 和朴素贝叶斯之类的算法比，对数据没有假设，准确度高，对异常点不敏感

　　　　5） 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合
    
　　　　6）该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分
　　　　

　　　　KNN的主要缺点有：

　　　　1）计算量大，尤其是特征数非常多的时候

　　　　2）样本不平衡的时候，对稀有类别的预测准确率低

　　　　3）KD树，球树之类的模型建立需要大量的内存

　　　　4）使用懒散学习方法，基本上不学习，导致预测时速度比起逻辑回归之类的算法慢

　　　　5）相比决策树模型，KNN模型可解释性不强
 
  - 参考
    - <a href = "https://www.cnblogs.com/pinard/p/6061661.html">K近邻法(KNN)原理小结</a>
